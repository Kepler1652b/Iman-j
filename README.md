# Persian Text Similarity & Summarization Toolkit

## ğŸ“– Overview
This project provides a lightweight toolkit for **Scraping and Preprocessing Cinema news with telegram bot interface**.  
It includes modules for:

- **Scraping News Websites** 
- **DataBase Handling**
- **Scheduled Bot for posting news**
- **Text preprocessing** (normalization, tokenization, stopword removal)
- **TFâ€‘IDF calculation** (Term Frequencyâ€“Inverse Document Frequency)
- **Cosine similarity** between Persian texts
- **Extractive text summarization** using Parsivar


---

## âœ¨ Features

- **Felexible Scraper Container**
  - Centralized scraper handling via ScraperContainer

- **Text Preprocessing**
  - Lowercasing and cleaning nonâ€‘Persian characters
  - Normalizing Persian digits (`Û°Û±Û²Û³...Û¹`) into Arabic numerals (`0â€“9`)
  - Tokenization into words and sentences
  - Stopword removal

- **TFâ€‘IDF & Similarity**
  - Compute TF and IDF values across a corpus
  - Generate TFâ€‘IDF vectors
  - Measure cosine similarity between two texts

- **Summarization**
  - Normalize and tokenize text into sentences
  - Extract a percentage of sentences (`ratio`)
  - Limit summary length (`sentence_limit`)
  - Output concise summaries of Persian documents

---

## âš™ï¸ Installation

### Requirements
- Python 3.x
- Libraries:
  - `math`, `re`, `collections`, `typing`
  - [Parsivar](https://github.com/ICTRC/Parsivar)


### Install Dependencies
```bash
uv sync 
pip install -r requirements.txt
```

Example `requirements.txt`:
```txt
parsivar==0.3
```
Example `pyproject.toml`:
```txt
dependencies = [
    "beautifulsoup4>=4.14.3",
    "feedparser>=6.0.12",
    "httpx>=0.28.1",
    "pytest>=9.0.1",
    "pytest-cov>=7.0.0",
    "pytest-mock>=3.15.1",
    "python-telegram-bot>=22.5",
    "sqlmodel>=0.0.27",
]
```
---

## ğŸš€ Usage

### Scraping 
```python
from scraper.scraper import ScraperContianer,extract_data,parser_data
from httpx import Client
if __name__ == "__main__":
    session = Client()
    contianer = ScraperContianer()
    scraper = contianer.resolve("<scraper_map scraper name>",session=session)
    data = extract_data(scraper)
    parsed_data_list = parser_data(scraper,data)
    details = scraper.detail_parser(parsed_data_list) 
```
### Text Preprocessing
```python
from persian_text_similarity import TextProcessor

text = "Ø§ÛŒÙ† ÛŒÚ© Ù…ØªÙ† Ù†Ù…ÙˆÙ†Ù‡ Ø§Ø³Øª."
cleaned = TextProcessor.preprocess_text(text)
tokens = TextProcessor.tokenize(cleaned)
tokens_no_stopwords = TextProcessor.remove_stopwords(tokens)
print(tokens_no_stopwords)
```

### TFâ€‘IDF & Cosine Similarity
```python
from persian_text_similarity import TFIDFCalculator, SimilarityCalculator

corpus = [tokens_no_stopwords, ["Ø³ÛŒÙ†Ù…Ø§", "Ù‡Ù†Ø±", "ØªØ§Ø±ÛŒØ®"]]
idf = TFIDFCalculator.calculate_idf(corpus)
tfidf = TFIDFCalculator.calculate_tf_idf(corpus, idf)

similarity = SimilarityCalculator.cosine_similarity(tfidf[0], tfidf[1])
print(f"Cosine similarity: {similarity}")
```

### Text Summarization
```python
from persian_text_summarizer import TextSummarizationPipeline

input_text = """Ø³ÛŒÙ†Ù…Ø§ ÛŒÚ©ÛŒ Ø§Ø² Ù…Ù‡Ù…â€ŒØªØ±ÛŒÙ† Ù‡Ù†Ø±Ù‡Ø§ÛŒ Ù‚Ø±Ù† Ø¨ÛŒØ³ØªÙ… Ø§Ø³Øª..."""
pipeline = TextSummarizationPipeline(input_text, ratio=0.3, sentence_limit=5)
pipeline.process_and_summarize()
```

---

## ğŸ“ Example

```python
from persian_text_similarity import TextSimilarity

text1 = "ØªØ§Ø±ÛŒØ® Ø³ÛŒÙ†Ù…Ø§: Ø§Ø² Ø¯ÙˆØ±Ø§Ù† ØµØ§Ù…Øª ØªØ§ ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒ Ø¨Ù„Ø§Ú©â€ŒØ¨Ø§Ø³ØªØ± Ø§Ù…Ø±ÙˆØ²ÛŒ..."
text2 = "ØªØ§Ø±ÛŒØ®Ú†Ù‡ Ø³ÛŒÙ†Ù…Ø§: Ø§Ø² ÙÛŒÙ„Ù…â€ŒÙ‡Ø§ÛŒ ØµØ§Ù…Øª ØªØ§ Ø¨Ù„Ø§Ú©â€ŒØ¨Ø§Ø³ØªØ±Ù‡Ø§ÛŒ Ù…Ø¯Ø±Ù†..."

similarity_calc = TextSimilarity(text1, text2)
similarity = similarity_calc.process_and_calculate_similarity()

print(f"Cosine similarity between the two texts: {similarity}")
```

### Scraping 
```python
from scraper.scraper import ScraperContianer,extract_data,parser_data
from httpx import Client
if __name__ == "__main__":
    session = Client()
    contianer = ScraperContianer()
    sc_list = ['moviemag',]
    for sc in sc_list:
        scraper = contianer.resolve(sc,session=session)
        data = extract_data(scraper)
        parsed_data_list = parser_data(scraper,data)
        details = scraper.detail_parser(parsed_data_list) 
```
---

## ğŸ“‚ File Structure
```
your-project/
â”œâ”€â”€ LICENSE
â”œâ”€â”€ bot                          # Telegram bot interface
    â”œâ”€â”€ templates                   # Templat folder for layout preset
        â”œâ”€â”€ admin.py                    # Admin template
        â”œâ”€â”€ base.py                     # Base Template for Button and layout
        â”œâ”€â”€ post.py                     # End point post template
    â”œâ”€â”€ bot_utilities.py            # Bot utilities
    â”œâ”€â”€ bot.py                      # Bot Main Code logic
    â”œâ”€â”€ commands.py                 # Bot Commands 
â”œâ”€â”€ database                    # DataBase 
    â”œâ”€â”€ db.py                       # Database CRUD actions for Tables 
    â”œâ”€â”€ models.py                   # Database Tables schemas 
â”œâ”€â”€ persian_nlp_tools           # Preprocessing Persian text
    â”œâ”€â”€ LICENSE
    â”œâ”€â”€ persian_text_similarity.py   # Preprocessing, TF-IDF, similarity
    â”œâ”€â”€ persian_text_summarizer.py   # Summarization logic
    â”œâ”€â”€ README.md                    # Documentation
    â”œâ”€â”€ requirements.txt             # Dependencies
â”œâ”€â”€ scraper                      # Web Scraper 
    â”œâ”€â”€ scraper_utilities.py        # Scraper utilities
    â”œâ”€â”€ scraper.py                  # Scraper Classes 
â”œâ”€â”€ .gitignore
â”œâ”€â”€ .python-version
â”œâ”€â”€ main.py                      # Main File to Run 
â”œâ”€â”€ pyproject.toml               # Dependencies
â”œâ”€â”€ pytest.ini
â”œâ”€â”€ README.md                    
â”œâ”€â”€ uv.lock                      # Dependencies
```

---

## ğŸ¤ Contributing
Contributions are welcome!  
1. Fork the repository  
2. Create a new branch for your feature or fix  
3. Add tests to cover your changes  
4. Submit a pull request  

---

## ğŸ“œ License
This project is openâ€‘source under the [MIT License](LICENSE).

---


